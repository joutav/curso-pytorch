{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Carregamento de Dados II.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDM84H0wO5I_",
        "colab_type": "text"
      },
      "source": [
        "# Carregamento de Dados\n",
        "\n",
        "Objetivos dessa aula:\n",
        "* Carregar um dataset customizado\n",
        "* Implementar o fluxo de treinamento **e validação** completo de uma rede\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_W2-tDlpz3",
        "colab_type": "text"
      },
      "source": [
        "## Hiperparâmetros\n",
        "\n",
        "Vamos manter a organização do último script :)\n",
        "\n",
        "* imports de pacotes\n",
        "* configuração de hiperparâmetros\n",
        "* definição do hardware padrão utilizado\n",
        "\n",
        "E bora de GPU de novo! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfisIdsWlioB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70b1ec97-38f8-4123-b694-cad9352a80fb"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configurando hiperparâmetros.\n",
        "args = {\n",
        "    'epoch_num': 200,     # Número de épocas.\n",
        "    'lr': 5e-5,           # Taxa de aprendizado.\n",
        "    'weight_decay': 5e-4, # Penalidade L2 (Regularização).\n",
        "    'num_workers': 0,     # Número de threads do dataloader.\n",
        "    'batch_size': 20,     # Tamanho do batch.\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyXDulgH77_s",
        "colab_type": "text"
      },
      "source": [
        "## Dataset \n",
        "\n",
        "Dataset de aplicativos para aluguel de bicicletas (*Bike Sharing Dataset*). <br>\n",
        "* Dadas algumas informações como velocidade do vento, estação do ano, etc., quantas bicicletas serão alugadas na próxima hora?\n",
        "\n",
        "Esse é um problema de **Regressão**, onde precisamos estimar uma variável dependente em um espaço contínuo (alugueis de bikes) a partir de um conjunto de variáveis independentes (as condições no momento)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ydTqRvqH-1-",
        "colab_type": "text"
      },
      "source": [
        "### Baixando o dataset\n",
        "\n",
        "Fonte: https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecgKLh2aKYH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "d2e9f410-2796-4127-c845-42315aa66c72"
      },
      "source": [
        "url =  'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('dados/bicicletas/bike-sharing.zip', 'wb') as f:\n",
        "    f.write(r.content)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('dados/bicicletas/bike-sharing.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dados/bicicletas/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjsCxBQJsTce",
        "colab_type": "text"
      },
      "source": [
        "### Visualizando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUkvnM8SKlY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "96916e77-bf6d-4dfa-eac8-524495c4c5e1"
      },
      "source": [
        "data_frame = pd.read_csv('dados/bicicletas/hour.csv')\n",
        "data_frame"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       instant      dteday  season  yr  mnth  hr  holiday  weekday  \\\n",
              "0            1  2011-01-01       1   0     1   0        0        6   \n",
              "1            2  2011-01-01       1   0     1   1        0        6   \n",
              "2            3  2011-01-01       1   0     1   2        0        6   \n",
              "3            4  2011-01-01       1   0     1   3        0        6   \n",
              "4            5  2011-01-01       1   0     1   4        0        6   \n",
              "...        ...         ...     ...  ..   ...  ..      ...      ...   \n",
              "17374    17375  2012-12-31       1   1    12  19        0        1   \n",
              "17375    17376  2012-12-31       1   1    12  20        0        1   \n",
              "17376    17377  2012-12-31       1   1    12  21        0        1   \n",
              "17377    17378  2012-12-31       1   1    12  22        0        1   \n",
              "17378    17379  2012-12-31       1   1    12  23        0        1   \n",
              "\n",
              "       workingday  weathersit  temp   atemp   hum  windspeed  casual  \\\n",
              "0               0           1  0.24  0.2879  0.81     0.0000       3   \n",
              "1               0           1  0.22  0.2727  0.80     0.0000       8   \n",
              "2               0           1  0.22  0.2727  0.80     0.0000       5   \n",
              "3               0           1  0.24  0.2879  0.75     0.0000       3   \n",
              "4               0           1  0.24  0.2879  0.75     0.0000       0   \n",
              "...           ...         ...   ...     ...   ...        ...     ...   \n",
              "17374           1           2  0.26  0.2576  0.60     0.1642      11   \n",
              "17375           1           2  0.26  0.2576  0.60     0.1642       8   \n",
              "17376           1           1  0.26  0.2576  0.60     0.1642       7   \n",
              "17377           1           1  0.26  0.2727  0.56     0.1343      13   \n",
              "17378           1           1  0.26  0.2727  0.65     0.1343      12   \n",
              "\n",
              "       registered  cnt  \n",
              "0              13   16  \n",
              "1              32   40  \n",
              "2              27   32  \n",
              "3              10   13  \n",
              "4               1    1  \n",
              "...           ...  ...  \n",
              "17374         108  119  \n",
              "17375          81   89  \n",
              "17376          83   90  \n",
              "17377          48   61  \n",
              "17378          37   49  \n",
              "\n",
              "[17379 rows x 17 columns]"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instant</th>\n      <th>dteday</th>\n      <th>season</th>\n      <th>yr</th>\n      <th>mnth</th>\n      <th>hr</th>\n      <th>holiday</th>\n      <th>weekday</th>\n      <th>workingday</th>\n      <th>weathersit</th>\n      <th>temp</th>\n      <th>atemp</th>\n      <th>hum</th>\n      <th>windspeed</th>\n      <th>casual</th>\n      <th>registered</th>\n      <th>cnt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.81</td>\n      <td>0.0000</td>\n      <td>3</td>\n      <td>13</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0000</td>\n      <td>8</td>\n      <td>32</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.22</td>\n      <td>0.2727</td>\n      <td>0.80</td>\n      <td>0.0000</td>\n      <td>5</td>\n      <td>27</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0000</td>\n      <td>3</td>\n      <td>10</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2011-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.24</td>\n      <td>0.2879</td>\n      <td>0.75</td>\n      <td>0.0000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>17374</th>\n      <td>17375</td>\n      <td>2012-12-31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>19</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.26</td>\n      <td>0.2576</td>\n      <td>0.60</td>\n      <td>0.1642</td>\n      <td>11</td>\n      <td>108</td>\n      <td>119</td>\n    </tr>\n    <tr>\n      <th>17375</th>\n      <td>17376</td>\n      <td>2012-12-31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>20</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.26</td>\n      <td>0.2576</td>\n      <td>0.60</td>\n      <td>0.1642</td>\n      <td>8</td>\n      <td>81</td>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>17376</th>\n      <td>17377</td>\n      <td>2012-12-31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>21</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2576</td>\n      <td>0.60</td>\n      <td>0.1642</td>\n      <td>7</td>\n      <td>83</td>\n      <td>90</td>\n    </tr>\n    <tr>\n      <th>17377</th>\n      <td>17378</td>\n      <td>2012-12-31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>22</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2727</td>\n      <td>0.56</td>\n      <td>0.1343</td>\n      <td>13</td>\n      <td>48</td>\n      <td>61</td>\n    </tr>\n    <tr>\n      <th>17378</th>\n      <td>17379</td>\n      <td>2012-12-31</td>\n      <td>1</td>\n      <td>1</td>\n      <td>12</td>\n      <td>23</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.26</td>\n      <td>0.2727</td>\n      <td>0.65</td>\n      <td>0.1343</td>\n      <td>12</td>\n      <td>37</td>\n      <td>49</td>\n    </tr>\n  </tbody>\n</table>\n<p>17379 rows × 17 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcvrTUl2OLmL",
        "colab_type": "text"
      },
      "source": [
        "### Tratamento de dados\n",
        "\n",
        "**Variáveis Categóricas** <br>\n",
        "Como descrito na página do dataset, apenas as variáveis numéricas estão normalizadas. No caso das categóricas (como dia da semana e estação do ano), cada elemento contém o índice da categoria.\n",
        "\n",
        "Existem várias formas de lidar com variáveis categóricas em uma regressão, mas para não desviar o foco da nossa aula manteremos os valores originais das variáveis categóricas.\n",
        "\n",
        "**Separação em treino e teste**<br>\n",
        "\n",
        "Para treinar e validar o nosso modelo, precisamos de dois conjuntos de dados (treino e teste). Para isso, utilizaremos a função ```torch.randperm``` para amostrar aleatoriamente um percentual dos dados, separando-os para validação.\n",
        "\n",
        "Documentação: https://pytorch.org/docs/stable/torch.html#torch.randperm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnjCdm1bMqhG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "7c307c94-e1bd-433c-955d-712da1f20feb"
      },
      "source": [
        "df_train, df_test = train_test_split(data_frame, test_size=0.2)\n",
        "print(f'Treino: {df_train.shape[0]}, Teste: {df_test.shape[0]}')\n",
        "\n",
        "df_train.to_csv('dados/bicicletas/df_train.csv', index=False)\n",
        "df_test.to_csv('dados/bicicletas/df_test.csv', index=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treino: 13903, Teste: 3476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daUL9nMxICW0",
        "colab_type": "text"
      },
      "source": [
        "### Classe Dataset\n",
        "\n",
        "O pacote ```torch.util.data``` possui a classe abstrata ```Dataset```. Ela permite que você implemente o seu próprio dataset reescrevendo os métodos:\n",
        "\n",
        "* ```__init__(self)```: Define a lista de amostras do seu dataset\n",
        "* ```__getitem__(self, idx)```: Carrega uma amostra, aplica as devidas transformações e retorna uma **tupla ```(dado, rótulo)```**.\n",
        "* ```__len__(self)```: Retorna a quantidade de amostras do dataset\n",
        "\n",
        "Tutorial completo do PyTorch: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaLPZteS9fkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bicicletas(Dataset):\n",
        "  def __init__(self, csv_path):\n",
        "\n",
        "    self.dados = pd.read_csv(csv_path).to_numpy()\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    sample = self.dados[idx][2:14]\n",
        "    label = self.dados[idx][-1:]\n",
        "    \n",
        "    # Converte para tensor\n",
        "    features = torch.from_numpy(sample.astype(np.float32))\n",
        "    label = torch.from_numpy(label.astype(np.float32))\n",
        "\n",
        "    sample = (features, label)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dados)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd4yl7sbQQGr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eaf1f6e9-af4c-4302-e780-a453d99c4411"
      },
      "source": [
        "train_set = Bicicletas('dados/bicicletas/df_train.csv')\n",
        "test_set = Bicicletas('dados/bicicletas/df_test.csv')\n",
        "\n",
        "dado, rotulo = train_set[0]\n",
        "print(rotulo)\n",
        "print(dado)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([86.])\ntensor([ 4.0000,  0.0000, 10.0000,  6.0000,  0.0000,  1.0000,  1.0000,  1.0000,\n         0.2400,  0.2879,  0.8700,  0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwS8q0l0v8cP",
        "colab_type": "text"
      },
      "source": [
        "### Construindo conjuntos de treino e teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G8nAUzhxUlrZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "344cf7b1-8650-42d0-a941-472a05b53921"
      },
      "source": [
        "print('Tamanho do treino: ' + str(len(train_set)) + ' amostras')\n",
        "print('Tamanho do teste: ' + str(len(test_set)) + ' amostras')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do treino: 13903 amostras\nTamanho do teste: 3476 amostras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZq6iuq6lQ9N",
        "colab_type": "text"
      },
      "source": [
        "## Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuETOc64MynK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando dataloader\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size = args['batch_size'],\n",
        "                          num_workers=args['num_workers'],\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         batch_size = args['batch_size'],\n",
        "                         num_workers=args['num_workers'],\n",
        "                         shuffle=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 12])\ntorch.Size([20, 1])\n"
          ]
        }
      ],
      "source": [
        "for batch in train_loader:\n",
        "    dado, label = batch\n",
        "    print(dado.size())\n",
        "    print(label.size())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_wBx0Uesrgu",
        "colab_type": "text"
      },
      "source": [
        "O objeto retornado é um **iterador**, podendo ser utilizado para iterar em loops mas não suportando indexação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPCE1MC8yf02",
        "colab_type": "text"
      },
      "source": [
        "## Implementando o MLP\n",
        "\n",
        "Essa parte aqui você já tira de letra! Minha sugestão é construir um modelo com:\n",
        "\n",
        "* **Duas camadas escondidas**. Lembre-se de alternar as camadas com ativações não-lineares. \n",
        "* Uma camada de saída (com qual ativação?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-9spUHCUzBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ff71e483-f77b-4086-feac-a60be74625a5"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  \n",
        "  def __init__(self, input_size, hidden_size, out_size):\n",
        "    super(MLP, self).__init__()\n",
        "    \n",
        "    self.features = nn.Sequential(\n",
        "          nn.Linear(input_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_size, hidden_size),\n",
        "          nn.ReLU(),\n",
        "    )\n",
        "    \n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(hidden_size, out_size),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    \n",
        "    hidden = self.features(X)\n",
        "    output = self.classifier(hidden)\n",
        "    \n",
        "    return output\n",
        "\n",
        "input_size  = train_set[0][0].size(0)\n",
        "hidden_size = 128\n",
        "out_size    = 1\n",
        "\n",
        "net = MLP(input_size, hidden_size, out_size).to(args['device'])\n",
        "print(net)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n  (features): Sequential(\n    (0): Linear(in_features=12, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=128, bias=True)\n    (3): ReLU()\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=128, out_features=1, bias=True)\n    (1): ReLU()\n  )\n)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb7zTsWV1cyQ",
        "colab_type": "text"
      },
      "source": [
        "## Definindo loss e otimizador\n",
        "\n",
        "Se lembra quais as funções de perda adequadas para um problema de regressão?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx4MecnX1e2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.L1Loss().to(args['device'])\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ujnT7rl0bjg",
        "colab_type": "text"
      },
      "source": [
        "# Fluxo de Treinamento & Validação\n",
        "\n",
        "## Treinamento\n",
        "\n",
        "Relembrando o passo a passo do fluxo de treinamento:\n",
        "* Iterar nas épocas\n",
        "* Iterar nos batches\n",
        "* Cast dos dados no dispositivo de hardware\n",
        "* Forward na rede e cálculo da loss\n",
        "* Cálculo do gradiente e atualização dos pesos\n",
        "\n",
        "Esse conjunto de passos é responsável pelo processo iterativo de otimização de uma rede. **A validação** por outro lado, é apenas a aplicação da rede em dados nunca antes visto para estimar a qualidade do modelo no mundo real.\n",
        "\n",
        "## Validação\n",
        "\n",
        "Para essa etapa, o PyTorch oferece dois artifícios:\n",
        "* ```model.eval()```: Impacta no *forward* da rede, informando as camadas caso seu comportamento mude entre fluxos (ex: dropout).\n",
        "* ```with torch.no_grad()```: Gerenciador de contexto que desabilita o cálculo e armazenamento de gradientes (economia de tempo e memória). Todo o código de validação deve ser executado dentro desse contexto.\n",
        "\n",
        "Exemplo de código para validação\n",
        "\n",
        "```python\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "  for batch in test_loader:\n",
        "      # Código de validação\n",
        "```\n",
        "\n",
        "\n",
        "Existe o equivalente ao ```model.eval()``` para explicitar que a sua rede deve estar em modo de treino, é o ```model.train()```. Apesar de ser o padrão dos modelos, é boa prática definir também o modo de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUwXgLyM4V2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, net, epoch):\n",
        "    net.train()\n",
        "    epoch_loss = list()\n",
        "    for batch in train_loader:\n",
        "\n",
        "        dado, rotulo = batch\n",
        "\n",
        "        # Cast GPU\n",
        "        dado = dado.to(args['device'])\n",
        "        rotulo = rotulo.to(args['device'])\n",
        "\n",
        "        # Forward\n",
        "        predicao = net(dado)\n",
        "        loss = criterion(predicao, rotulo)\n",
        "        epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "        # Backward \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = np.asarray(epoch_loss)\n",
        "    print(f'Época {epoch}, Loss: {epoch_loss.mean()} +/- {epoch_loss.std()}, Tempo: ')\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79KDmwyL4l89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(test_loader, net, epoch):\n",
        "\n",
        "  # Evaluation mode\n",
        "  net.eval()\n",
        "  \n",
        "  start = time.time()\n",
        "  \n",
        "  epoch_loss  = []\n",
        "  \n",
        "  with torch.no_grad(): \n",
        "    for batch in test_loader:\n",
        "\n",
        "      dado, rotulo = batch\n",
        "\n",
        "      # Cast do dado na GPU\n",
        "      dado = dado.to(args['device'])\n",
        "      rotulo = rotulo.to(args['device'])\n",
        "\n",
        "      # Forward\n",
        "      ypred = net(dado)\n",
        "      loss = criterion(ypred, rotulo)\n",
        "      epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  \n",
        "  end = time.time()\n",
        "  print('********** Validate **********')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), end-start))\n",
        "  \n",
        "  return epoch_loss.mean()\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95dKe7by6Qot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e48a5af3-413a-4e41-bf06-5abc79915450",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "for epoch in range(args['epoch_num']):\n",
        "  train(train_loader, net, epoch)\n",
        "  validate(test_loader, net, epoch)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 64, Loss: 77.1151351928711 +/- 22.043760299682617, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 64, Loss: 77.4810 +/- 22.5750, Time: 0.14\n",
            "\n",
            "Época 65, Loss: 75.9288330078125 +/- 19.61199951171875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 65, Loss: 73.9550 +/- 19.2312, Time: 0.14\n",
            "\n",
            "Época 66, Loss: 75.61204528808594 +/- 22.748016357421875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 66, Loss: 73.6869 +/- 20.1000, Time: 0.15\n",
            "\n",
            "Época 67, Loss: 74.74018859863281 +/- 18.52726936340332, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 67, Loss: 73.7058 +/- 20.5276, Time: 0.18\n",
            "\n",
            "Época 68, Loss: 74.63904571533203 +/- 23.575101852416992, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 68, Loss: 74.0031 +/- 18.8825, Time: 0.20\n",
            "\n",
            "Época 69, Loss: 74.53748321533203 +/- 20.380413055419922, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 69, Loss: 76.4153 +/- 21.8237, Time: 0.22\n",
            "\n",
            "Época 70, Loss: 74.44606018066406 +/- 20.827392578125, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 70, Loss: 74.8941 +/- 18.5859, Time: 0.17\n",
            "\n",
            "Época 71, Loss: 74.16983032226562 +/- 20.403533935546875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 71, Loss: 72.9611 +/- 20.3299, Time: 0.14\n",
            "\n",
            "Época 72, Loss: 73.45783996582031 +/- 19.673782348632812, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 72, Loss: 73.0751 +/- 20.5972, Time: 0.17\n",
            "\n",
            "Época 73, Loss: 73.54154968261719 +/- 21.722719192504883, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 73, Loss: 74.7598 +/- 18.6151, Time: 0.21\n",
            "\n",
            "Época 74, Loss: 73.62188720703125 +/- 20.95234489440918, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 74, Loss: 75.6759 +/- 21.9771, Time: 0.31\n",
            "\n",
            "Época 75, Loss: 73.20321655273438 +/- 18.818824768066406, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 75, Loss: 72.5662 +/- 18.8559, Time: 0.16\n",
            "\n",
            "Época 76, Loss: 72.59393310546875 +/- 21.873493194580078, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 76, Loss: 71.7053 +/- 19.2649, Time: 0.16\n",
            "\n",
            "Época 77, Loss: 72.32698822021484 +/- 19.90222930908203, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 77, Loss: 73.9911 +/- 21.4749, Time: 0.18\n",
            "\n",
            "Época 78, Loss: 72.74095916748047 +/- 20.979713439941406, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 78, Loss: 72.4653 +/- 18.3605, Time: 0.17\n",
            "\n",
            "Época 79, Loss: 72.09575653076172 +/- 21.13076400756836, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 79, Loss: 70.2036 +/- 18.9910, Time: 0.18\n",
            "\n",
            "Época 80, Loss: 71.78692626953125 +/- 18.618270874023438, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 80, Loss: 72.7920 +/- 20.9351, Time: 0.16\n",
            "\n",
            "Época 81, Loss: 71.79754638671875 +/- 20.114662170410156, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 81, Loss: 70.9110 +/- 18.5311, Time: 0.14\n",
            "\n",
            "Época 82, Loss: 70.95938873291016 +/- 21.087186813354492, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 82, Loss: 70.1095 +/- 18.8164, Time: 0.15\n",
            "\n",
            "Época 83, Loss: 71.2363052368164 +/- 20.14779281616211, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 83, Loss: 72.2419 +/- 21.1372, Time: 0.22\n",
            "\n",
            "Época 84, Loss: 70.74861145019531 +/- 19.067420959472656, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 84, Loss: 68.6728 +/- 19.1162, Time: 0.24\n",
            "\n",
            "Época 85, Loss: 69.86723327636719 +/- 20.95574951171875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 85, Loss: 69.9289 +/- 18.0945, Time: 0.17\n",
            "\n",
            "Época 86, Loss: 69.93771362304688 +/- 20.618982315063477, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 86, Loss: 69.2842 +/- 19.6559, Time: 0.15\n",
            "\n",
            "Época 87, Loss: 69.66756439208984 +/- 18.953052520751953, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 87, Loss: 70.7093 +/- 20.3592, Time: 0.16\n",
            "\n",
            "Época 88, Loss: 70.165771484375 +/- 19.684207916259766, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 88, Loss: 69.6674 +/- 18.2714, Time: 0.17\n",
            "\n",
            "Época 89, Loss: 69.53852081298828 +/- 20.08275032043457, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 89, Loss: 68.0071 +/- 18.2821, Time: 0.18\n",
            "\n",
            "Época 90, Loss: 69.73735809326172 +/- 19.693634033203125, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 90, Loss: 71.5317 +/- 20.8448, Time: 0.30\n",
            "\n",
            "Época 91, Loss: 69.75292205810547 +/- 19.362512588500977, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 91, Loss: 67.4296 +/- 18.0105, Time: 0.16\n",
            "\n",
            "Época 92, Loss: 69.41484069824219 +/- 21.7081356048584, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 92, Loss: 68.4692 +/- 18.2722, Time: 0.18\n",
            "\n",
            "Época 93, Loss: 69.4140853881836 +/- 19.84551429748535, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 93, Loss: 71.2041 +/- 20.7514, Time: 0.21\n",
            "\n",
            "Época 94, Loss: 68.78535461425781 +/- 19.7222843170166, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 94, Loss: 69.7675 +/- 17.8999, Time: 0.19\n",
            "\n",
            "Época 95, Loss: 68.31025695800781 +/- 19.1962890625, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 95, Loss: 67.4978 +/- 19.4727, Time: 0.18\n",
            "\n",
            "Época 96, Loss: 67.35994720458984 +/- 18.065229415893555, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 96, Loss: 66.1907 +/- 18.5881, Time: 0.18\n",
            "\n",
            "Época 97, Loss: 67.25224304199219 +/- 19.788867950439453, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 97, Loss: 68.2480 +/- 17.8299, Time: 0.17\n",
            "\n",
            "Época 98, Loss: 67.79203796386719 +/- 19.319107055664062, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 98, Loss: 68.1535 +/- 19.6402, Time: 0.17\n",
            "\n",
            "Época 99, Loss: 66.80976867675781 +/- 18.64812469482422, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 99, Loss: 66.2295 +/- 18.4844, Time: 0.18\n",
            "\n",
            "Época 100, Loss: 66.63925170898438 +/- 19.184673309326172, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 100, Loss: 67.0277 +/- 17.4302, Time: 0.18\n",
            "\n",
            "Época 101, Loss: 66.02222442626953 +/- 19.767431259155273, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 101, Loss: 67.0504 +/- 19.6067, Time: 0.18\n",
            "\n",
            "Época 102, Loss: 65.86925506591797 +/- 18.613914489746094, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 102, Loss: 65.0799 +/- 18.3055, Time: 0.19\n",
            "\n",
            "Época 103, Loss: 66.38972473144531 +/- 19.36359214782715, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 103, Loss: 66.5680 +/- 17.7008, Time: 0.18\n",
            "\n",
            "Época 104, Loss: 66.53659057617188 +/- 18.938446044921875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 104, Loss: 66.0310 +/- 19.4495, Time: 0.22\n",
            "\n",
            "Época 105, Loss: 65.24799346923828 +/- 18.810691833496094, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 105, Loss: 64.0300 +/- 17.5740, Time: 0.23\n",
            "\n",
            "Época 106, Loss: 64.94388580322266 +/- 18.565553665161133, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 106, Loss: 65.5471 +/- 17.8004, Time: 0.32\n",
            "\n",
            "Época 107, Loss: 64.45738983154297 +/- 18.125167846679688, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 107, Loss: 64.7440 +/- 18.6124, Time: 0.16\n",
            "\n",
            "Época 108, Loss: 63.8221435546875 +/- 18.652265548706055, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 108, Loss: 64.2147 +/- 18.4155, Time: 0.15\n",
            "\n",
            "Época 109, Loss: 63.544471740722656 +/- 17.70907211303711, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 109, Loss: 63.8844 +/- 16.9511, Time: 0.14\n",
            "\n",
            "Época 110, Loss: 63.26734161376953 +/- 17.704238891601562, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 110, Loss: 63.8746 +/- 17.4644, Time: 0.16\n",
            "\n",
            "Época 111, Loss: 63.27134323120117 +/- 17.799936294555664, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 111, Loss: 63.1944 +/- 17.7734, Time: 0.18\n",
            "\n",
            "Época 112, Loss: 63.07699203491211 +/- 18.679553985595703, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 112, Loss: 63.6186 +/- 18.3771, Time: 0.26\n",
            "\n",
            "Época 113, Loss: 63.0880126953125 +/- 17.774429321289062, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 113, Loss: 62.6539 +/- 17.7190, Time: 0.22\n",
            "\n",
            "Época 114, Loss: 62.95127868652344 +/- 18.158185958862305, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 114, Loss: 63.4959 +/- 16.6617, Time: 0.22\n",
            "\n",
            "Época 115, Loss: 62.529239654541016 +/- 17.16120719909668, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 115, Loss: 62.2637 +/- 17.2278, Time: 0.19\n",
            "\n",
            "Época 116, Loss: 62.191802978515625 +/- 17.001615524291992, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 116, Loss: 63.2055 +/- 17.8207, Time: 0.19\n",
            "\n",
            "Época 117, Loss: 62.22003936767578 +/- 18.23099136352539, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 117, Loss: 61.5503 +/- 17.0946, Time: 0.16\n",
            "\n",
            "Época 118, Loss: 61.8292236328125 +/- 17.89496612548828, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 118, Loss: 62.7489 +/- 16.4442, Time: 0.29\n",
            "\n",
            "Época 119, Loss: 61.91087341308594 +/- 17.280303955078125, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 119, Loss: 61.4506 +/- 16.6564, Time: 0.15\n",
            "\n",
            "Época 120, Loss: 61.43834686279297 +/- 17.393856048583984, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 120, Loss: 62.6437 +/- 17.7915, Time: 0.16\n",
            "\n",
            "Época 121, Loss: 61.29706573486328 +/- 17.70935821533203, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 121, Loss: 61.7813 +/- 16.0612, Time: 0.18\n",
            "\n",
            "Época 122, Loss: 60.88237380981445 +/- 17.594131469726562, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 122, Loss: 61.5716 +/- 16.1587, Time: 0.16\n",
            "\n",
            "Época 123, Loss: 60.655364990234375 +/- 17.34534454345703, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 123, Loss: 61.1343 +/- 16.9181, Time: 0.17\n",
            "\n",
            "Época 124, Loss: 60.422847747802734 +/- 16.213470458984375, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 124, Loss: 61.3585 +/- 16.9266, Time: 0.18\n",
            "\n",
            "Época 125, Loss: 60.2628288269043 +/- 15.787885665893555, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 125, Loss: 61.4968 +/- 17.2330, Time: 0.19\n",
            "\n",
            "Época 126, Loss: 60.14500427246094 +/- 17.176971435546875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 126, Loss: 60.5819 +/- 16.7284, Time: 0.17\n",
            "\n",
            "Época 127, Loss: 59.816062927246094 +/- 17.028215408325195, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 127, Loss: 60.5609 +/- 16.4792, Time: 0.16\n",
            "\n",
            "Época 128, Loss: 59.6596794128418 +/- 16.602283477783203, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 128, Loss: 59.9177 +/- 15.9525, Time: 0.17\n",
            "\n",
            "Época 129, Loss: 59.47550582885742 +/- 17.09698486328125, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 129, Loss: 59.6196 +/- 15.9596, Time: 0.17\n",
            "\n",
            "Época 130, Loss: 59.26036071777344 +/- 16.249549865722656, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 130, Loss: 59.3507 +/- 16.4894, Time: 0.18\n",
            "\n",
            "Época 131, Loss: 58.80405807495117 +/- 16.263084411621094, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 131, Loss: 58.9529 +/- 16.0250, Time: 0.29\n",
            "\n",
            "Época 132, Loss: 58.63227462768555 +/- 16.72870445251465, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 132, Loss: 58.4790 +/- 16.1657, Time: 0.15\n",
            "\n",
            "Época 133, Loss: 58.34333038330078 +/- 16.38893699645996, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 133, Loss: 58.3923 +/- 16.3812, Time: 0.18\n",
            "\n",
            "Época 134, Loss: 58.33951950073242 +/- 15.902701377868652, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 134, Loss: 58.2289 +/- 16.2127, Time: 0.17\n",
            "\n",
            "Época 135, Loss: 58.22636032104492 +/- 16.586143493652344, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 135, Loss: 58.0113 +/- 16.2401, Time: 0.19\n",
            "\n",
            "Época 136, Loss: 57.96534729003906 +/- 16.374792098999023, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 136, Loss: 58.0508 +/- 15.8440, Time: 0.18\n",
            "\n",
            "Época 137, Loss: 58.16195297241211 +/- 15.603315353393555, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 137, Loss: 57.8002 +/- 15.8564, Time: 0.17\n",
            "\n",
            "Época 138, Loss: 57.952484130859375 +/- 15.982457160949707, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 138, Loss: 57.6386 +/- 15.7713, Time: 0.18\n",
            "\n",
            "Época 139, Loss: 57.96665573120117 +/- 15.174766540527344, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 139, Loss: 57.8597 +/- 15.8768, Time: 0.17\n",
            "\n",
            "Época 140, Loss: 57.637168884277344 +/- 15.790926933288574, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 140, Loss: 58.6315 +/- 15.7390, Time: 0.17\n",
            "\n",
            "Época 141, Loss: 57.973453521728516 +/- 16.49315643310547, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 141, Loss: 59.4582 +/- 15.7825, Time: 0.19\n",
            "\n",
            "Época 142, Loss: 58.76472854614258 +/- 16.02978515625, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 142, Loss: 56.9719 +/- 15.9055, Time: 0.15\n",
            "\n",
            "Época 143, Loss: 58.6864128112793 +/- 16.012399673461914, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 143, Loss: 60.5033 +/- 17.5292, Time: 0.21\n",
            "\n",
            "Época 144, Loss: 58.104801177978516 +/- 16.484447479248047, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 144, Loss: 57.0601 +/- 16.2793, Time: 0.16\n",
            "\n",
            "Época 145, Loss: 58.41837692260742 +/- 16.172176361083984, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 145, Loss: 59.0396 +/- 15.7268, Time: 0.19\n",
            "\n",
            "Época 146, Loss: 58.67070770263672 +/- 16.147747039794922, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 146, Loss: 56.8878 +/- 16.4416, Time: 0.21\n",
            "\n",
            "Época 147, Loss: 58.07899475097656 +/- 16.161293029785156, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 147, Loss: 58.2673 +/- 16.9562, Time: 0.19\n",
            "\n",
            "Época 148, Loss: 58.09663009643555 +/- 16.692419052124023, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 148, Loss: 58.1722 +/- 16.2972, Time: 0.22\n",
            "\n",
            "Época 149, Loss: 57.55494689941406 +/- 15.98474407196045, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 149, Loss: 56.9058 +/- 16.0654, Time: 0.14\n",
            "\n",
            "Época 150, Loss: 57.753726959228516 +/- 16.299535751342773, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 150, Loss: 58.5227 +/- 16.9015, Time: 0.14\n",
            "\n",
            "Época 151, Loss: 56.814796447753906 +/- 15.949203491210938, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 151, Loss: 56.7078 +/- 16.3142, Time: 0.15\n",
            "\n",
            "Época 152, Loss: 56.9708366394043 +/- 15.247549057006836, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 152, Loss: 56.8444 +/- 15.9934, Time: 0.15\n",
            "\n",
            "Época 153, Loss: 56.74789047241211 +/- 16.506263732910156, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 153, Loss: 57.4080 +/- 15.6070, Time: 0.31\n",
            "\n",
            "Época 154, Loss: 56.29948043823242 +/- 15.81402587890625, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 154, Loss: 56.3127 +/- 16.1928, Time: 0.26\n",
            "\n",
            "Época 155, Loss: 56.12893295288086 +/- 15.83289909362793, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 155, Loss: 57.4344 +/- 16.5568, Time: 0.17\n",
            "\n",
            "Época 156, Loss: 56.51636505126953 +/- 16.157419204711914, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 156, Loss: 56.8659 +/- 16.4625, Time: 0.17\n",
            "\n",
            "Época 157, Loss: 56.10846710205078 +/- 15.391351699829102, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 157, Loss: 56.8083 +/- 16.2590, Time: 0.25\n",
            "\n",
            "Época 158, Loss: 55.86865234375 +/- 15.416074752807617, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 158, Loss: 55.8138 +/- 15.9634, Time: 0.20\n",
            "\n",
            "Época 159, Loss: 55.820098876953125 +/- 16.30406379699707, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 159, Loss: 56.1615 +/- 16.2551, Time: 0.19\n",
            "\n",
            "Época 160, Loss: 55.7606201171875 +/- 15.807647705078125, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 160, Loss: 56.2646 +/- 16.0794, Time: 0.16\n",
            "\n",
            "Época 161, Loss: 56.313621520996094 +/- 16.44536018371582, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 161, Loss: 56.3307 +/- 15.7888, Time: 0.14\n",
            "\n",
            "Época 162, Loss: 56.19690704345703 +/- 15.928460121154785, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 162, Loss: 56.1033 +/- 16.1125, Time: 0.15\n",
            "\n",
            "Época 163, Loss: 56.04817199707031 +/- 15.710868835449219, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 163, Loss: 57.0274 +/- 16.3714, Time: 0.12\n",
            "\n",
            "Época 164, Loss: 56.31308364868164 +/- 15.087034225463867, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 164, Loss: 57.4606 +/- 15.9063, Time: 0.27\n",
            "\n",
            "Época 165, Loss: 56.508888244628906 +/- 15.581812858581543, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 165, Loss: 56.0797 +/- 16.0354, Time: 0.26\n",
            "\n",
            "Época 166, Loss: 57.035560607910156 +/- 15.903218269348145, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 166, Loss: 57.2504 +/- 15.7211, Time: 0.25\n",
            "\n",
            "Época 167, Loss: 56.53315734863281 +/- 15.932887077331543, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 167, Loss: 58.6019 +/- 15.6086, Time: 0.15\n",
            "\n",
            "Época 168, Loss: 57.32724380493164 +/- 16.33816146850586, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 168, Loss: 56.8497 +/- 16.0638, Time: 0.15\n",
            "\n",
            "Época 169, Loss: 56.425376892089844 +/- 15.341282844543457, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 169, Loss: 56.7974 +/- 16.3444, Time: 0.17\n",
            "\n",
            "Época 170, Loss: 56.503761291503906 +/- 15.328656196594238, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 170, Loss: 55.7780 +/- 15.3001, Time: 0.20\n",
            "\n",
            "Época 171, Loss: 55.4771614074707 +/- 16.091796875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 171, Loss: 56.2381 +/- 15.7614, Time: 0.17\n",
            "\n",
            "Época 172, Loss: 56.15465545654297 +/- 15.816515922546387, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 172, Loss: 55.7177 +/- 16.2582, Time: 0.16\n",
            "\n",
            "Época 173, Loss: 55.35183334350586 +/- 15.460556983947754, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 173, Loss: 55.5279 +/- 16.0704, Time: 0.13\n",
            "\n",
            "Época 174, Loss: 55.56106185913086 +/- 15.91080093383789, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 174, Loss: 55.3118 +/- 15.6442, Time: 0.15\n",
            "\n",
            "Época 175, Loss: 55.698448181152344 +/- 16.44332504272461, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 175, Loss: 56.4138 +/- 15.1011, Time: 0.15\n",
            "\n",
            "Época 176, Loss: 55.46558380126953 +/- 14.830719947814941, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 176, Loss: 55.0104 +/- 15.4221, Time: 0.13\n",
            "\n",
            "Época 177, Loss: 55.47340774536133 +/- 14.563490867614746, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 177, Loss: 56.5109 +/- 15.8643, Time: 0.13\n",
            "\n",
            "Época 178, Loss: 55.32365798950195 +/- 14.840352058410645, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 178, Loss: 55.4269 +/- 15.9658, Time: 0.17\n",
            "\n",
            "Época 179, Loss: 55.27866744995117 +/- 16.010238647460938, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 179, Loss: 56.2658 +/- 15.1480, Time: 0.16\n",
            "\n",
            "Época 180, Loss: 54.92749786376953 +/- 15.18210220336914, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 180, Loss: 55.3804 +/- 15.6890, Time: 0.16\n",
            "\n",
            "Época 181, Loss: 55.08259201049805 +/- 14.46729850769043, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 181, Loss: 56.3507 +/- 15.7639, Time: 0.14\n",
            "\n",
            "Época 182, Loss: 54.89089584350586 +/- 15.765632629394531, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 182, Loss: 55.7914 +/- 15.9230, Time: 0.13\n",
            "\n",
            "Época 183, Loss: 55.336029052734375 +/- 15.450043678283691, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 183, Loss: 56.3545 +/- 14.8405, Time: 0.13\n",
            "\n",
            "Época 184, Loss: 55.070003509521484 +/- 15.238499641418457, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 184, Loss: 55.9955 +/- 15.4687, Time: 0.17\n",
            "\n",
            "Época 185, Loss: 54.57832717895508 +/- 14.755606651306152, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 185, Loss: 55.4734 +/- 15.1890, Time: 0.20\n",
            "\n",
            "Época 186, Loss: 54.62565231323242 +/- 15.179866790771484, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 186, Loss: 56.0645 +/- 16.0441, Time: 0.16\n",
            "\n",
            "Época 187, Loss: 54.46421432495117 +/- 14.622638702392578, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 187, Loss: 54.3685 +/- 15.1255, Time: 0.15\n",
            "\n",
            "Época 188, Loss: 54.70159912109375 +/- 14.98639965057373, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 188, Loss: 54.4035 +/- 14.8863, Time: 0.12\n",
            "\n",
            "Época 189, Loss: 54.50397491455078 +/- 15.503376960754395, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 189, Loss: 54.5562 +/- 14.8793, Time: 0.13\n",
            "\n",
            "Época 190, Loss: 54.727325439453125 +/- 14.978846549987793, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 190, Loss: 53.5345 +/- 15.0917, Time: 0.19\n",
            "\n",
            "Época 191, Loss: 54.313621520996094 +/- 14.1435546875, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 191, Loss: 54.2026 +/- 15.3375, Time: 0.16\n",
            "\n",
            "Época 192, Loss: 54.11566162109375 +/- 14.623886108398438, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 192, Loss: 54.8132 +/- 15.1105, Time: 0.16\n",
            "\n",
            "Época 193, Loss: 53.77750778198242 +/- 15.750752449035645, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 193, Loss: 55.0020 +/- 15.2872, Time: 0.12\n",
            "\n",
            "Época 194, Loss: 54.06165313720703 +/- 14.760595321655273, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 194, Loss: 55.9811 +/- 15.0186, Time: 0.12\n",
            "\n",
            "Época 195, Loss: 54.00092697143555 +/- 14.65272045135498, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 195, Loss: 54.5921 +/- 15.1022, Time: 0.17\n",
            "\n",
            "Época 196, Loss: 54.6162223815918 +/- 15.035699844360352, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 196, Loss: 56.0761 +/- 15.9829, Time: 0.17\n",
            "\n",
            "Época 197, Loss: 54.14728546142578 +/- 15.054191589355469, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 197, Loss: 54.1011 +/- 14.9635, Time: 0.13\n",
            "\n",
            "Época 198, Loss: 54.06233215332031 +/- 15.030540466308594, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 198, Loss: 55.1915 +/- 14.8168, Time: 0.15\n",
            "\n",
            "Época 199, Loss: 53.81659698486328 +/- 14.887649536132812, Tempo: \n",
            "********** Validate **********\n",
            "Epoch 199, Loss: 53.2469 +/- 14.9616, Time: 0.15\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I5L8uae15qz",
        "colab_type": "text"
      },
      "source": [
        "# Gráfico de convergência"
      ]
    }
  ]
}